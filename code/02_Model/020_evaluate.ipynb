{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copyright (C) Microsoft Corporation.  \n",
    "see license.md file for Enterprise Customer License and ISV License details "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow multiple displays per cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# try:\n",
    "#     amlWBSharedDir = os.environ['AZUREML_NATIVE_SHARE_DIRECTORY']    \n",
    "# except:\n",
    "#     print( 'Not in amlwb? define amlWBSharedDir as \"/shared_folder_on_host/amlwb_exp_acc/amlwb_work_space/amlwb_experiment\"')\n",
    "    \n",
    "# amlWBSharedDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the Azure Machine Learning data collector to log various metrics\n",
    "# from azureml.logging import get_azureml_logger\n",
    "# logger = get_azureml_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Azure Machine Learning history magic to control history collection\n",
    "# History is off by default, options are \"on\", \"off\", or \"show\"\n",
    "# %azureml history on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import utlity functions\n",
    "\n",
    "paths_to_append = [os.path.join(os.getcwd(), os.path.join(*([ '..', 'src'])))]\n",
    "def add_path_to_sys_path(path_to_append):\n",
    "    if not (any(path_to_append in paths for paths in sys.path)):\n",
    "        sys.path.append(path_to_append)\n",
    "[add_path_to_sys_path(crt_path) for crt_path in paths_to_append]\n",
    "\n",
    "import azure_chestxray_utils\n",
    "import azure_chestxray_keras_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the file path variables \n",
    "# paths are tipically container level dirs mapped to a host dir for data persistence.\n",
    "\n",
    "prj_consts = azure_chestxray_utils.chestxray_consts()\n",
    "\n",
    "data_base_input_dir=os.path.join(os.path.join(*([os.sep]+prj_consts.BASE_INPUT_DIR_list)))\n",
    "data_base_output_dir=os.path.join( os.path.join(*([os.sep]+prj_consts.BASE_OUTPUT_DIR_list))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/azureml-share/chestxray/output/weights_tmpdir'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/azureml-share/chestxray/output/fully_trained_models'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 86320\r\n",
      "-rw-rw-r-- 1 1003 1003 30097832 Feb 14 04:37 azure_chest_xray_14_weights_712split_epoch_054_val_loss_191.2588.hdf5\r\n",
      "-rw-r--r-- 1 root root 29143128 Feb 14 04:55 weights_only_azure_chest_xray_14_weights_712split_epoch_054_val_loss_191.2588.hdf5\r\n",
      "-rw-rw-r-- 1 1003 1003 29142168 Feb  7 06:16 weights_only_azure_chest_xray__14_weights_712split_epoch_029_val_loss_147.7599.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "\n",
    "#location of trained models weights, quality will be dependent on train data size\n",
    "# and number of epochs among other things\n",
    "weights_dir = os.path.join(data_base_output_dir, os.path.join(*(prj_consts.MODEL_WEIGHTS_DIR_list))) \n",
    "weights_dir\n",
    "!ls -l {weights_dir}\n",
    "\n",
    "# \"quality\" models, fully trained on all training data\n",
    "fully_trained_weights_dir = os.path.join(data_base_output_dir, os.path.join(*(prj_consts.FULLY_PRETRAINED_MODEL_DIR_list))) \n",
    "fully_trained_weights_dir\n",
    "!ls -l {fully_trained_weights_dir}\n",
    "\n",
    "\n",
    "nih_chest_xray_data_dir=os.path.join(data_base_input_dir, \n",
    "                                     os.path.join(*(prj_consts.ChestXray_IMAGES_DIR_list)))\n",
    "\n",
    "data_partitions_dir=os.path.join(data_base_output_dir, \n",
    "                                os.path.join(*(prj_consts.DATA_PARTITIONS_DIR_list)))  \n",
    "label_path = os.path.join(data_partitions_dir,'labels14_unormalized_cleaned.pickle')\n",
    "partition_path = os.path.join(data_partitions_dir, 'partition14_unormalized_cleaned.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # extract and save the weights from a full model\n",
    "\n",
    "# import keras_contrib\n",
    "# from keras.models import load_model\n",
    "# model_file_name = 'azure_chest_xray_14_weights_712split_epoch_054_val_loss_191.2588.hdf5'\n",
    "# model = load_model(os.path.join(fully_trained_weights_dir, model_file_name))\n",
    "# model.save_weights(os.path.join(fully_trained_weights_dir, 'weights_only_'+model_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/azureml-share/chestxray/output/fully_trained_models/azure_chest_xray_14_weights_712split_epoch_054_val_loss_191.2588.hdf5',\n",
       " '/azureml-share/chestxray/output/fully_trained_models/weights_only_azure_chest_xray_14_weights_712split_epoch_054_val_loss_191.2588.hdf5',\n",
       " '/azureml-share/chestxray/output/fully_trained_models/weights_only_azure_chest_xray__14_weights_712split_epoch_029_val_loss_147.7599.hdf5']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get long (full path) model file name\n",
    "\n",
    "all_models=!ls {os.path.join(fully_trained_weights_dir, '*.hdf5')}\n",
    "all_models\n",
    "models_file_name= [os.path.join(fully_trained_weights_dir, \n",
    "                               'weights_only_azure_chest_xray_14_weights_712split_epoch_054_val_loss_191.2588.hdf5')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from keras.utils import Sequence\n",
    "from sklearn import metrics\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"  # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "resized_height = 224\n",
    "resized_width = 224\n",
    "num_channel = 3\n",
    "num_classes = 14\n",
    "batch_size = 512 #512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/device:GPU:0', '/device:GPU:1']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "def get_available_gpus():\n",
    "    \"\"\"\n",
    "    Returns: number of GPUs available in the system\n",
    "    \"\"\"\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "get_available_gpus()\n",
    "# get number of available GPUs\n",
    "print(\"num of GPUs:\", len(get_available_gpus()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device_lib.list_local_devices()\n",
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of GPUs: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Atelectasis',\n",
       " 'Cardiomegaly',\n",
       " 'Effusion',\n",
       " 'Infiltration',\n",
       " 'Mass',\n",
       " 'Nodule',\n",
       " 'Pneumonia',\n",
       " 'Pneumothorax',\n",
       " 'Consolidation',\n",
       " 'Edema',\n",
       " 'Emphysema',\n",
       " 'Fibrosis',\n",
       " 'Pleural Thickening',\n",
       " 'Hernia']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_gpu = get_available_gpus()\n",
    "# get number of available GPUs\n",
    "print(\"num of GPUs:\", len(get_available_gpus()))\n",
    "\n",
    "pathologies_name_list = prj_consts.DISEASE_list\n",
    "pathologies_name_list\n",
    "\n",
    "stanford_result = [0.8094, 0.9248, 0.8638, 0.7345, 0.8676, 0.7802, 0.7680, 0.8887, 0.7901, 0.8878, 0.9371, 0.8047,\n",
    "                   0.8062, 0.9164]\n",
    "\n",
    "\n",
    "with open(label_path, 'rb') as f:\n",
    "    labels = pickle.load(f)\n",
    "\n",
    "with open(partition_path, 'rb') as f:\n",
    "    partition = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# generator for train and validation data\n",
    "# use the Sequence class per issue https://github.com/keras-team/keras/issues/1638\n",
    "class DataGenSequence(Sequence):\n",
    "    def __init__(self, labels, image_file_index, current_state):\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.img_file_index = image_file_index\n",
    "        self.current_state = current_state\n",
    "        self.len = len(self.img_file_index) // self.batch_size\n",
    "        print(\"for DataGenSequence\", current_state, \"total rows are:\", len(self.img_file_index), \", len is\", self.len)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # print(\"loading data segmentation\", idx)\n",
    "        # make sure each batch size has the same amount of data\n",
    "        current_batch = self.img_file_index[idx * self.batch_size: (idx + 1) * self.batch_size]\n",
    "        X = np.empty((self.batch_size, resized_height, resized_width, num_channel))\n",
    "        y = np.empty((self.batch_size, num_classes))\n",
    "\n",
    "        for i, image_name in enumerate(current_batch):\n",
    "            path = os.path.join(nih_chest_xray_data_dir, image_name)\n",
    "\n",
    "            # loading data\n",
    "\n",
    "            img = cv2.resize(cv2.imread(path), (resized_height, resized_width)).astype(np.float32)\n",
    "            X[i, :, :, :] = img\n",
    "            y[i, :] = labels[image_name]\n",
    "\n",
    "            # only do random flipping in training status\n",
    "        if self.current_state == 'train':\n",
    "            # this is different from the training code\n",
    "            x_augmented = X\n",
    "        else:\n",
    "            x_augmented = X\n",
    "\n",
    "        return x_augmented, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32893"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partition['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of result is 32768\n",
      "/azureml-share/chestxray/output/fully_trained_models/weights_only_azure_chest_xray_14_weights_712split_epoch_054_val_loss_191.2588.hdf5\n",
      "Weights for the model were loaded successfully\n",
      "evaluation for model /azureml-share/chestxray/output/fully_trained_models/weights_only_azure_chest_xray_14_weights_712split_epoch_054_val_loss_191.2588.hdf5\n",
      "for DataGenSequence test total rows are: 32893 , len is 64\n",
      "64/64 [==============================] - 469s 7s/step\n",
      "result shape (32768, 14)\n",
      "               Disease  Our AUC Score  Stanford AUC Score     Delta\n",
      "0          Atelectasis       0.823191              0.8094 -0.013791\n",
      "1         Cardiomegaly       0.933519              0.9248 -0.008719\n",
      "2             Effusion       0.883184              0.8638 -0.019384\n",
      "3         Infiltration       0.744561              0.7345 -0.010061\n",
      "4                 Mass       0.859510              0.8676  0.008090\n",
      "5               Nodule       0.783997              0.7802 -0.003797\n",
      "6            Pneumonia       0.801597              0.7680 -0.033597\n",
      "7         Pneumothorax       0.830550              0.8887  0.058150\n",
      "8        Consolidation       0.813993              0.7901 -0.023893\n",
      "9                Edema       0.896173              0.8878 -0.008373\n",
      "10           Emphysema       0.849184              0.9371  0.087916\n",
      "11            Fibrosis       0.882463              0.8047 -0.077763\n",
      "12  Pleural Thickening       1.000000              0.8062 -0.193800\n",
      "13              Hernia       0.916395              0.9164  0.000005\n"
     ]
    }
   ],
   "source": [
    "import keras_contrib\n",
    "\n",
    "# load test data\n",
    "X_test = np.empty((len(partition['test']), 224, 224, 3), dtype=np.float32)\n",
    "y_test = np.empty((len(partition['test']) - len(partition['test']) % batch_size, 14), dtype=np.float32)\n",
    "\n",
    "for i, npy in enumerate(partition['test']):\n",
    "    if (i < len(y_test)):\n",
    "        # round to batch_size\n",
    "        y_test[i, :] = labels[npy]\n",
    "\n",
    "print(\"len of result is\", len(y_test))\n",
    "y_pred_list = np.empty((len(models_file_name), len(partition['test']), 14), dtype=np.float32)\n",
    "\n",
    "# individual models\n",
    "for index, current_model_file in enumerate(models_file_name):\n",
    "    print(current_model_file)\n",
    "#    model = load_model(current_model_file)\n",
    "    model = azure_chestxray_keras_utils.build_model(keras_contrib.applications.densenet.DenseNetImageNet121); model.load_weights(current_model_file)\n",
    "\n",
    "    print('evaluation for model', current_model_file)\n",
    "    # y_pred = model.predict(X_test)\n",
    "\n",
    "    y_pred = model.predict_generator(generator=DataGenSequence(labels, partition['test'], current_state='test'),\n",
    "                                     workers=32, verbose=1, max_queue_size=1)\n",
    "    print(\"result shape\", y_pred.shape)\n",
    "    \n",
    "    # add one fake row of ones in both test and pred values to avoid:\n",
    "    # ValueError: Only one class present in y_true. ROC AUC score is not defined in that case.\n",
    "    y_test = np.insert(y_test, 0, np.ones((y_test.shape[1],)), 0)\n",
    "    y_pred = np.insert(y_pred, 0, np.ones((y_pred.shape[1],)), 0)\n",
    "\n",
    "    df = pd.DataFrame(columns=['Disease', 'Our AUC Score', 'Stanford AUC Score'])\n",
    "    for d in range(14):\n",
    "        df.loc[d] = [pathologies_name_list[d],\n",
    "                     metrics.roc_auc_score(y_test[:, d], y_pred[:, d]),\n",
    "                     stanford_result[d]]\n",
    "\n",
    "    df['Delta'] = df['Stanford AUC Score'] - df['Our AUC Score']\n",
    "    df.to_csv(current_model_file + \".csv\", index=False)\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 32893, 14)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(32769, 14)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([4004, 1189, 5187, 7044, 2089, 2090,  593, 2450, 1850,  808,  933,\n",
       "        493,    1,   79])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_list.shape\n",
    "type(y_test[:, d])\n",
    "y_test.shape\n",
    "y_test[:3,]\n",
    "y_test.sum(axis=0).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C:\\repos\\ChestXRay\\TDSP>jupyter nbconvert --to html .\\Code\\02_Model\\020_evaluate.ipynb\n",
    "# [NbConvertApp] Converting notebook .\\Code\\01_DataPrep\\001_get_data.ipynb to html\n",
    "# [NbConvertApp] Writing 263414 bytes to .\\Code\\01_DataPrep\\001_get_data.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
